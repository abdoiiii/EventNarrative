{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd04aca4c1cae7485cc41799a731ba5854b53d97568598fe9b096575a558693a0f1",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "224428\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "path = \"../data/split_data/\"\n",
    "filelist = [file_ for file_ in os.listdir(path) if file_.endswith('.json')]\n",
    "data_list = []\n",
    "\n",
    "\n",
    "for file_ in filelist:\n",
    "    # data_list = []\n",
    "    # print(file_)\n",
    "    # curr_file = file_.split('.')[0]\n",
    "    with open(path+file_, 'r', encoding='utf-8') as files:\n",
    "        for line in files:\n",
    "            event = json.loads(line.strip('\\n'))\n",
    "                # types = set()\n",
    "                # for triple in event['triples']:\n",
    "                #     if ('wikidata_type_label' == triple[1] or 'instance of' == triple[1]) and (triple[0] == event['Event_Name']) and (triple[2] != 'Wikimedia disambiguation page'):\n",
    "            #         types.add(triple[2])\n",
    "            # event['types']= list(types)\n",
    "            # keys = ['Event_Name','keep_triples', 'narration', 'entity_ref_dict', 'types','wikipediaLabel']\n",
    "            # filtered_event = dict((k, event[k]) for k in keys if k in event)\n",
    "            data_list.append(event)\n",
    "print(len(data_list))\n",
    "    # with open(path+curr_file+'_data.json', 'w', encoding='utf-8') as fout:\n",
    "    #     json.dump(data_list , fout)    \n",
    "# print(len(wikipediaTitles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Event_Name': \"1948 Governor General's Awards\",\n",
       " 'keep_triples': [[\"1948 Governor General's Awards\", 'point in time', '1948'],\n",
       "  [\"1948 Governor General's Awards\", 'country', 'Canada'],\n",
       "  [\"1948 Governor General's Awards\", 'facet of', \"Governor General's Awards\"],\n",
       "  [\"1948 Governor General's Awards\",\n",
       "   'part of the series',\n",
       "   \"Governor General's Awards\"]],\n",
       " 'narration': \"The <entity_0> were the 13th rendition of the <entity_1>, <entity_2>'s annual national awards program which then comprised literary awards alone. The awards recognized Canadian writers for new English-language works published in <entity_2> during <entity_3> and were presented early in 1949.\",\n",
       " 'entity_ref_dict': {'<entity_0>': \"1948 Governor General's Awards\",\n",
       "  '<entity_1>': \"Governor General's Awards\",\n",
       "  '<entity_2>': 'Canada',\n",
       "  '<entity_3>': '1948'},\n",
       " 'types': ['award ceremony'],\n",
       " 'wikipediaLabel': \"1948_Governor_General's_Awards\"}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_triples = []\n",
    "for data in data_list:\n",
    "    if data['keep_triples'] is not None:\n",
    "        all_triples.append(data['keep_triples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "40720"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "len(all_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# RELATIONS:  7\n# ENTITIES:  159691\n"
     ]
    }
   ],
   "source": [
    "relations = set()\n",
    "entities = set()\n",
    "type_relations = defaultdict(list)\n",
    "relations_count = dict()\n",
    "for data in data_list:\n",
    "    triples = data['keep_triples']\n",
    "    for triple in triples:\n",
    "        relations.add(triple[1])\n",
    "        entities.add(triple[0])\n",
    "        entities.add(triple[2])\n",
    "#     triples = data['triples']\n",
    "#     for triple in triples:\n",
    "#          if ('wikidata_type_label' == triple[1] or 'instance of' == triple[1]) and (triple[0] == data['Event_Name']) and (triple[2] != 'Wikimedia disambiguation page'):\n",
    "#             if triple[2] in relations_count:\n",
    "#                 relations_count[triple[2]] +=1\n",
    "#             else:\n",
    "#                 relations_count[triple[2]] = 1\n",
    "# relations_count = Counter(relations_count)\n",
    "print('# RELATIONS: ', len(relations))\n",
    "print('# ENTITIES: ', len(entities))\n",
    "# print('TOP TYPES: ', relations_count.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of relations:  95219\n"
     ]
    }
   ],
   "source": [
    "types = set()\n",
    "for data in data_list:\n",
    "    triples = data['triples']\n",
    "    for triple in triples:\n",
    "            types.add(triple[2])\n",
    "print(\"Number of types: \", len(types))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "relations_list = []\n",
    "for data in data_list:\n",
    "    triples = data['keep_triples']\n",
    "    for triple in triples:\n",
    "            relation = triple[1]\n",
    "            relations_list.append(relation)\n",
    "test = Counter(relations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('point in time', 113551),\n",
       " ('location', 97209),\n",
       " ('country', 62577),\n",
       " ('sport', 51081),\n",
       " ('start time', 46742),\n",
       " ('instance of', 45724),\n",
       " ('end time', 38364),\n",
       " ('part of', 23716),\n",
       " ('sports season of league or competition', 18769),\n",
       " ('winner', 12127)]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "test.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_relations = defaultdict(set)\n",
    "for data in data_list:\n",
    "    relations = set()\n",
    "    types = set()\n",
    "    for triple in data['triples']:\n",
    "        relations.add(triple[1])\n",
    "        if ('wikidata_type_label' == triple[1] or 'instance of' == triple[1]) and (triple[0] == data['Event_Name']) and (triple[2] != 'Wikimedia disambiguation page'):\n",
    "            types.add(triple[2])\n",
    "    for typ in types:\n",
    "        type_relations[typ].update(relations)\n",
    "type_relations = defaultdict(list, ((k, list(v)) for k, v in type_relations.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "40720\n# TRIPLES:  180603\nAVG TRIPLES:  4.435240667976425\nSTD:  3.1863048904829623\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "all_triples = list()\n",
    "for data in data_list:\n",
    "    triples = data['keep_triples']\n",
    "    all_triples.append(len(triples))\n",
    "print(len(data_list))\n",
    "print(\"# TRIPLES: \", sum(all_triples))\n",
    "print(\"AVG TRIPLES: \", statistics.mean(all_triples))\n",
    "print(\"STD: \", statistics.stdev(all_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_text(text, entity_ref_dict):\n",
    "    filled_text = text\n",
    "    for ref in entity_ref_dict:\n",
    "        filled_text = filled_text.replace(ref, entity_ref_dict[ref])\n",
    "    return filled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# TOKENS:  5760660\nAVG TOKENS:  141.47003929273083\nSTD:  40.92397781513411\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokens_len_dict = dict()\n",
    "tokens_len = []\n",
    "for idx, data in enumerate(data_list):\n",
    "    text =  data['abstract_og']\n",
    "    # text = fill_text(text, data['entity_ref_dict'])\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    # tokenized_list = filled_text.split(' ')\n",
    "    num_tokens = len(tokenized_text)\n",
    "    # tokens_len_dict[data['wikipediaLabel']] = num_tokens\n",
    "    tokens_len.append(num_tokens)\n",
    "    # if num_tokens > 500:\n",
    "    #     data_list[idx] = None\n",
    "\n",
    "# print(len(data_list))\n",
    "print(\"# TOKENS: \", sum(tokens_len))\n",
    "print(\"AVG TOKENS: \", statistics.mean(tokens_len))\n",
    "print(\"STD: \", statistics.stdev(tokens_len))\n",
    "    # for token in tokenized_list:\n",
    "    #     tokens_list.append(token)\n",
    "# print(\"# TOKENS: \", len(tokens_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'new_data_list' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-c0c4d3cc9bc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokens_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entity_ref_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_data_list' is not defined"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# tokens_len = []\n",
    "# for idx, data in enumerate(new_data_list):\n",
    "#     text =  data['text']\n",
    "#     text = fill_text(text, data['entity_ref_dict'])\n",
    "#     tokenized_text = nltk.word_tokenize(text)\n",
    "#     # tokenized_list = filled_text.split(' ')\n",
    "#     num_tokens = len(tokenized_text)\n",
    "#     tokens_len.append(num_tokens)\n",
    "#     # tokens_len_dict[data['wikipediaLabel']] = num_tokens\n",
    "# print(\"# TOKENS: \", sum(tokens_len))\n",
    "# print(\"AVG Tokens: \", statistics.mean(tokens_len))\n",
    "# print(\"STD: \", statistics.stdev(tokens_len))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "metadata": {},
     "execution_count": 204
    }
   ],
   "source": [
    "max(tokens_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-5636f79f9cef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abstract_og'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# filled_text = fill_text(text, data['entity_ref_dict'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvocab_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# VOCAB: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_parentheses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTARTING_QUOTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocab_set = set()\n",
    "for data in data_list:\n",
    "    text =  data['abstract_og']\n",
    "    # filled_text = fill_text(text, data['entity_ref_dict'])\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    vocab_set.add(token)\n",
    "print(\"# VOCAB: \", len(vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# VOCAB:  77896\n"
     ]
    }
   ],
   "source": [
    "vocab_set = set()\n",
    "for data in data_list:\n",
    "    text =  data['abstract_og']\n",
    "    # text = tokenize_text(text)\n",
    "    # filled_text = fill_text(text, data['entity_ref_dict'])\n",
    "    tokenized_text = nltk.word_tokenize(text)   \n",
    "    for token in tokenized_text:\n",
    "        vocab_set.add(token)\n",
    "print(\"# VOCAB: \", len(vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# AVG (TRIPLES/TOKENS):  0.07460962438262746\n",
      "STD:  0.04999299333334047\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokens_per_triples = []\n",
    "for idx, data in enumerate(data_list):\n",
    "    text =  data['abstract']\n",
    "    filled_text = fill_text(text, data['entity_ref_dict'])\n",
    "    tokenized_text = nltk.word_tokenize(filled_text)\n",
    "    # tokenized_list = filled_text.split(' ')\n",
    "    num_tokens = len(tokenized_text)\n",
    "    num_triples = len(data['keep_triples'])\n",
    "    tokens_per_triples.append(num_triples/num_tokens)\n",
    "    # tokens_len_dict[data['wikipediaLabel']] = num_tokens\n",
    "print(\"# AVG (TRIPLES/TOKENS): \", statistics.mean(tokens_per_triples))\n",
    "print(\"STD: \", statistics.stdev(tokens_per_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# AVG (TRIPLES/Sentences):  0.8194457237261756\nSTD:  0.7473808655127846\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "triples_per_sentence = []\n",
    "for idx, data in enumerate(data_list):\n",
    "    text =  data['abstract_og']\n",
    "    # filled_text = fill_text(text, data['entity_ref_dict'])\n",
    "    sentences = sent_tokenize(text)\n",
    "    # tokenized_list = filled_text.split(' ')\n",
    "    num_sents = len(sentences)\n",
    "    num_triples = len(data['keep_triples'])\n",
    "    triples_per_sentence.append(num_triples/num_sents)\n",
    "print(\"# AVG (TRIPLES/Sentences): \", statistics.mean(triples_per_sentence))\n",
    "print(\"STD: \", statistics.stdev(triples_per_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entity_tokens(text, entity_ref_dict):\n",
    "    total = 0\n",
    "    for ref in entity_ref_dict:\n",
    "        number_of_matches = len(re.findall(ref, text))\n",
    "        entity = entity_ref_dict[ref]\n",
    "        len_of_entity = len(nltk.word_tokenize(entity))\n",
    "        entity_tokens_count = number_of_matches*len_of_entity\n",
    "        total += entity_tokens_count\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "% entities words in text:  0.2667982488117681\nAVG entities/words in text:  0.27015447010183635\nSTD:  0.07322821188494921\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "entities_len = []\n",
    "tokens_len = []\n",
    "percentage_list = []\n",
    "for idx, data in enumerate(data_list):\n",
    "    text =  data['abstract']\n",
    "    filled_text = fill_text(text, data['entity_ref_dict'])\n",
    "    tokenized_text = nltk.word_tokenize(filled_text)\n",
    "    num_entities = count_entity_tokens(text, data['entity_ref_dict'])\n",
    "    num_tokens = len(tokenized_text)\n",
    "    entities_len.append(num_entities)\n",
    "    tokens_len.append(num_tokens)\n",
    "    percentage_list.append((num_entities/num_tokens))\n",
    "print(\"% entities words in text: \", (sum(entities_len)/sum(tokens_len)))\n",
    "print(\"AVG entities/words in text: \", statistics.mean(percentage_list))\n",
    "print(\"STD: \", statistics.stdev(percentage_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get entities/tokens per sample\n",
    "#first fill text\n",
    "#then count how many tokens in that filled text are entities\n",
    "#divide by total number of tokens\n",
    "\n",
    "#iterate over entity_ref_dict\n",
    "#count how many of each in entity_ref_dict\n",
    "#3) tokenize entitity and multiply by how many\n",
    "#4) get tokens in each narrative\n",
    "#divide 3)/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get entities/tokens per sample\n",
    "#first fill text\n",
    "#then count how many entities in text overall over number of token words overall\n",
    "#divide by total number of tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(data_list):\n",
    "    text =  data['narration']\n",
    "    filled_text = fill_text(text, data['entity_ref_dict'])\n",
    "    found = re.findall(r'\\<entity[^>]*\\>', filled_text)\n",
    "    if found:\n",
    "        print(data)\n",
    "        print(found)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Battle of Zhuolu', 'point in time', '3000 BC']\n",
      "['Battle of Zhuolu', 'start time', '3999']\n",
      "['Battle of Zhuolu', 'end time', '31 December 3000']\n",
      "['Stuttgart 21', 'start time', '2025']\n",
      "['2030 FIFA World Cup', 'end time', '2030']\n",
      "['2030 FIFA World Cup', 'start time', '2030']\n",
      "['Great Flood', 'point in time', '3000 BC']\n",
      "['Great Flood', 'start time', '3999']\n",
      "['Great Flood', 'end time', '31 December 3000']\n",
      "['History of cricket in Pakistan from 1986 to 2000', 'end time', '31 December 2099']\n",
      "['International Decade for People of African Descent', 'end time', '2024']\n",
      "['Tropical Storm Luke', 'start time', '13 September 2050']\n",
      "['2024 Summer Olympics', 'start time', '26 July 2024']\n",
      "['2024 Summer Olympics', 'end time', '11 August 2024']\n",
      "['California Gold Rush', 'end time', '2056']\n",
      "['Khlysts', 'end time', '31 December 2099']\n",
      "['Vase of Entemena', 'start time', '2400']\n",
      "['UEFA Euro 2024', 'start time', '14 June 2024']\n",
      "['UEFA Euro 2024', 'end time', '14 July 2024']\n",
      "['solar eclipse of April 9, 2043', 'start time', '09 April 2043']\n",
      "['solar eclipse of April 9, 2043', 'end time', '09 April 2043']\n",
      "['2026 Winter Olympics', 'start time', '06 February 2026']\n",
      "['2026 Winter Olympics', 'end time', '22 February 2026']\n",
      "['Avellino eruption', 'start time', '2999']\n",
      "['Battle of Banquan', 'point in time', '3000 BC']\n",
      "['Battle of Banquan', 'start time', '3999']\n",
      "['Battle of Banquan', 'end time', '31 December 3000']\n",
      "['solar eclipse of June 13, 2132', 'start time', '13 June 2132']\n",
      "['solar eclipse of June 13, 2132', 'end time', '13 June 2132']\n",
      "['Q-School 2015 - Event 2', 'start time', '20 May 5015']\n"
     ]
    }
   ],
   "source": [
    "times = dict()\n",
    "for data in data_list:\n",
    "    triples = data['triples']\n",
    "    start_time = None\n",
    "    end_time = None\n",
    "    pot = None\n",
    "    for triple in triples:\n",
    "        if data['Event_Name'] == triple[0] and 'start time' in triple[1]:\n",
    "            split = triple[2].split(' ')\n",
    "            if 'BC' in triple[2]:\n",
    "                start_time = '-'+split[-2]\n",
    "                if int(start_time) == -3000:\n",
    "                    print(triple)\n",
    "                if int(start_time) <= 2043:\n",
    "                    times[data['wikipediaLabel']] = int(start_time)\n",
    "            else:\n",
    "                start_time = split[-1]\n",
    "                if int(start_time) == -3000:\n",
    "                    print(triple)\n",
    "                if int(start_time) >= 2024:\n",
    "                    print(triple)\n",
    "                    times[data['wikipediaLabel']] = int(start_time)\n",
    "        if data['Event_Name'] == triple[0] and 'end time' in triple[1]:\n",
    "            split = triple[2].split(' ')\n",
    "            if 'BC' in triple[2]:\n",
    "                end_time = '-'+split[-2]\n",
    "                if int(end_time) == -3000:\n",
    "                    print(triple)\n",
    "                if int(end_time) >= 2024:\n",
    "                    times[data['wikipediaLabel']] = int(end_time)\n",
    "                    print(triple)\n",
    "            else:\n",
    "                end_time = split[-1]\n",
    "                if int(end_time) == -3000:\n",
    "                    print(triple)\n",
    "                if int(end_time) >= 2024:\n",
    "                    times[data['wikipediaLabel']] = int(end_time)\n",
    "                    print(triple)\n",
    "        if data['Event_Name'] == triple[0] and 'point in time' in triple[1]:\n",
    "            split = triple[2].split(' ')\n",
    "            if 'BC' in triple[2]:\n",
    "                pot = '-'+split[-2]\n",
    "                if int(pot) == -3000:\n",
    "                    print(triple)\n",
    "                if int(pot) >= 2024:\n",
    "                    print(triple)\n",
    "                    times[data['wikipediaLabel']] = int(pot)\n",
    "            else:\n",
    "                pot = split[-1]\n",
    "                if not start_time and not end_time:\n",
    "                    times[data['wikipediaLabel']] = int(pot)\n",
    "                elif start_time and start_time != pot:\n",
    "                    if end_time:\n",
    "                        if end_time != pot:\n",
    "                            times[data['wikipediaLabel']] = int(pot)\n",
    "                    else:\n",
    "                        times[data['wikipediaLabel']] = int(pot)\n",
    "                elif end_time and end_time != pot:\n",
    "                    if start_time:\n",
    "                        if end_time != pot:\n",
    "                            times[data['wikipediaLabel']] = int(pot)\n",
    "                    else:\n",
    "                       times[data['wikipediaLabel']] = int(pot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_times = sorted(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-3000"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "source": [
    "sorted_times[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}